{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Fri. June 25 2021\\n\\n@author: Jane Hsieh\\n\\n@Folder stursure of data:\\n    data_folder (dir:origion_path)\\n    |__label1 folder\\n    |              |__ image11 file\\n    |              |__ image12 file\\n    |              | ...\\n    |               \\n    |__label2 folder \\n    |              |__ image21 file\\n    |              |__ image22 file\\n    |              | ...\\n    |\\n    |__label3 folder\\n    ...\\n@Issue:\\n    1. ResourceExhaustedError解决方法: https://blog.csdn.net/lics999/article/details/78517638\\n       --> type \"invidia-smi\" on terminal\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri. June 25 2021\n",
    "\n",
    "@author: Jane Hsieh\n",
    "\n",
    "@Folder stursure of data:\n",
    "    data_folder (dir:origion_path)\n",
    "    |__label1 folder\n",
    "    |              |__ image11 file\n",
    "    |              |__ image12 file\n",
    "    |              | ...\n",
    "    |               \n",
    "    |__label2 folder \n",
    "    |              |__ image21 file\n",
    "    |              |__ image22 file\n",
    "    |              | ...\n",
    "    |\n",
    "    |__label3 folder\n",
    "    ...\n",
    "@Issue:\n",
    "    1. ResourceExhaustedError解决方法: https://blog.csdn.net/lics999/article/details/78517638\n",
    "       --> type \"invidia-smi\" on terminal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/u4574403/.local/lib/python3.6/site-packages (21.3.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-gpu: [Errno 2] No such file or directory: '/home/u4574403/.local/lib/python3.6/site-packages/tensorflow_gpu-1.13.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/u4574403/.local/lib/python3.6/site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/u4574403/.local/lib/python3.6/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/u4574403/.local/lib/python3.6/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/u4574403/.local/lib/python3.6/site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/u4574403/.local/lib/python3.6/site-packages (from scikit-learn) (1.4.1)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-gpu: [Errno 2] No such file or directory: '/home/u4574403/.local/lib/python3.6/site-packages/tensorflow_gpu-1.13.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn \n",
    "# !pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /home/u4574403/.local/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /home/u4574403/.local/lib/python3.6/site-packages (from imblearn) (0.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/u4574403/.local/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/u4574403/.local/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/u4574403/.local/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/u4574403/.local/lib/python3.6/site-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/u4574403/.local/lib/python3.6/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.1.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-gpu: [Errno 2] No such file or directory: '/home/u4574403/.local/lib/python3.6/site-packages/tensorflow_gpu-1.13.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imblearn \n",
    "# !pip install imblearn --ignore-installed scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras==2.2.4 in /home/u4574403/.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/u4574403/.local/lib/python3.6/site-packages (from Keras==2.2.4) (1.1.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/u4574403/.local/lib/python3.6/site-packages (from Keras==2.2.4) (1.4.1)\n",
      "Requirement already satisfied: h5py in /home/u4574403/.local/lib/python3.6/site-packages (from Keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/u4574403/.local/lib/python3.6/site-packages (from Keras==2.2.4) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (5.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.0.8)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-gpu: [Errno 2] No such file or directory: '/home/u4574403/.local/lib/python3.6/site-packages/tensorflow_gpu-1.13.1.dist-info/METADATA'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==1.13.1 in /home/u4574403/.local/lib/python3.6/site-packages (1.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.13.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.8.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.19.5)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.48.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/u4574403/.local/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.37.1)\n",
      "Requirement already satisfied: h5py in /home/u4574403/.local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (49.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for tensorflow-gpu: [Errno 2] No such file or directory: '/home/u4574403/.local/lib/python3.6/site-packages/tensorflow_gpu-1.13.1.dist-info/METADATA'\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mexec_err_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f1f77d83adb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow==1.13.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow-gpu==1.13.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# !pip install tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# !pip install tensorflow-gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.13.1\n",
    "!pip install tensorflow-gpu==1.13.1\n",
    "\n",
    "# !pip install tensorflow\n",
    "# !pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless # !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-gdcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# troubleshooting: compatible with keara 2.2.4\n",
    "!pip install 'h5py==2.10.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import os\n",
    "from os import listdir, walk\n",
    "#from os.path import join\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## Analysis directory\n",
    "directory = \"/home/u4574403/Project_Jane/StemCellReg2021/Oral cancer three-class classification with SoftLabels (True Final 4 INTER)\"\n",
    "# \"D:\\Project_Jane\\StemCellReg2021\\Oral cancer three-class classification with SoftLabels (Prev INTER without 140 and 410)\"\n",
    "\n",
    "os.chdir(directory)\n",
    "os.getcwd()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "#import customized functions\n",
    "from CNN_Procedure import S1_Prep_RadomSplit as S1\n",
    "from CNN_Procedure import S2_TrainSet_Prep as S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Procedure import S3_Modeling as S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Procedure import S4_Evaluation as S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%                                        I. Load Raw Data & Train_Test Split\n",
    "#%%\n",
    "data_ver =  \"Hard with Soft_true_DS-OS\" #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "#\"Hard only\" #M1; Prep == None\n",
    "#\"Hard with Soft\" #M2; Prep == \"INTER\"\n",
    "#\"Hard with Soft_OS\" #M3; Prep== \"OS_INTER\"\n",
    "#Hard with Soft_B-SMOTE #M4; Prep == \"Bord_SMOTE_INTER\"\n",
    "#\"Hard with Soft_DS-OS\" #M5;  Prep == \"DS_INTER\"\n",
    "Prep = \"DS_INTER\"                #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "#%%                                        I.0 Input\n",
    "## Data directory: Hard (Val / Test)\n",
    "Val_folder = 'oral cancer 0521-0618_tag300_Val'\n",
    "Val_path = \"/home/u4574403/Project_Jane/StemCellReg2021/Data/\" + Val_folder\n",
    "#Define the splitted file names and their ratios (sum=1) for Val set\n",
    "Val_split_ratio = OrderedDict({'train': 1.0}) \n",
    "#if test set inculded in the same folder: split_ratio =  OrderedDict({'train': 0.80, 'test': 0.20})\n",
    "\n",
    "Test_folder = 'oral cancer 0521-0618_tag300_Test'\n",
    "Test_path = \"/home/u4574403/Project_Jane/StemCellReg2021/Data/\" + Test_folder\n",
    "Test_split_ratio = OrderedDict({'test': 1.0}) \n",
    "\n",
    "Labels = [\"control\", \"5nMTG\", 'NonCancerOral']\n",
    "Labels_dic ={}\n",
    "for i, label in enumerate(Labels):\n",
    "    Labels_dic[label] = i\n",
    "\n",
    "\n",
    "\n",
    "## Data directory: Intermediate Data (Val / Test) \n",
    "Val_INTER_folder = 'oral cancer inter stage-all_tag300_Val'\n",
    "Val_INTER_path = \"/home/u4574403/Project_Jane/StemCellReg2021/Data/\" + Val_INTER_folder\n",
    "Val_split_INTER_ratio = OrderedDict({'train': 0.8, 'val': 0.2}) \n",
    "#if test set inculded in the same folder: split_ratio =  OrderedDict({'train': 0.64, 'val': 0.16, 'test': 0.20})\n",
    "\n",
    "Test_INTER_folder = 'oral cancer inter stage-all_tag300_Test'\n",
    "Test_INTER_path = \"/home/u4574403/Project_Jane/StemCellReg2021/Data/\" + Test_INTER_folder\n",
    "Test_split_INTER_ratio = OrderedDict({'test': 1.0}) \n",
    "\n",
    "Labels_INTER = ['20X_TG2-WT1', '20X_TG1-WT2', '20X_TG3-WT1', '20X_TG1-WT3'] #['TG2-WT1_0709', '20X_TG2-WT1', '20X_TG1-WT2', '20X_TG3-WT1', '20X_TG1-WT3'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%                                        I.0 Input parameters for model training\n",
    "# For custom model parameters: loss, Activaion function\n",
    "Loss_name = 'mse' #'FocalLoss' \n",
    "Activation = 'softmax' #'softmax', 'sigmoid'\n",
    "\"\"\"\n",
    "for Loss_name = 'categorical_crossentropy'\n",
    "S3.create_EfficientNetB3(Loss=Loss_name)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#For integer m >=1, suppose the rartio - alpha0 : alpha1 : alph2 = m*x : x : x = (1-2/(m+2)) : 1/(m+2) : 1/(m+2)\n",
    "#From binary classification result, alpha0 : alpha1 = m*x : x where m~ 5~6\n",
    "\n",
    "gammas = [None]#[5, 5, 5]\n",
    "alpha_lists = [[None,None,None]]\n",
    "\n",
    "\n",
    "\n",
    "# For training set preprocessing\n",
    "'''\n",
    "if to resize image immediately in input stage (after Split_Data_Strafied): set \" resize = say, (W, H) = (300, 300)\"; o.w., None\n",
    "  -> preprocess pairs (resize, Prep):((300, 300), None) or (None, None), (None, \"CutMix\"), (None, \"Puzzle\")\n",
    "'''\n",
    "# Prep = None                  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "## optional\n",
    "if Prep== None:    ## M1:training with only hard labels \n",
    "    resize = (300, 300)\n",
    "    other_info = None\n",
    "if Prep== \"INTER\" or \"OS_INTER\": ##M2:\"INTER\": training with INTER (soft label)data;  M4:\"OS_INTER\": training with Over-Sampled INTER\n",
    "    resize = (300, 300)\n",
    "    other_info = None\n",
    "if Prep==\"CutMix_INTER\": #\"CutMix\":\n",
    "    resize = (300, 300)\n",
    "    CutMix_state = 2 # 0: conventilanl; 1: only same label CutMixed; 2: only different label CutMixed\n",
    "    other_info = CutMix_state  \n",
    "if Prep == \"SMOTE_INTER\":\n",
    "    resize = (300, 300)\n",
    "    SMOTE_state = 1 # 0: conventilanl; 1: only different label synthesized; \n",
    "    other_info = SMOTE_state\n",
    "if Prep == \"Bord_SMOTE_INTER\": #M3\n",
    "    resize = (300, 300)\n",
    "    other_info = None\n",
    "if Prep==\"Puzzle\":\n",
    "    resize = (300, 300) #for lung data: None;  for oral data: (1600, 1200)   # note: (W, H) for cv2 \n",
    "    len_crop = 100 # for Puzzle\n",
    "    other_info = len_crop\n",
    "if Prep== \"DS_INTER\": ##M5\"INTER\": training with INTER (soft label)data;  \"OS_INTER\": training with Over-Sampled INTER\n",
    "    resize = (300, 300)\n",
    "    other_info = \"OS\" #None#\n",
    "\n",
    "\n",
    "\n",
    "# for repetiontion\n",
    "Trials = 1\n",
    "K = 5\n",
    "\n",
    "# for modeling\n",
    "model_resize = None #input image size for model: EfficientNet B3\n",
    "batch_size = 16 #16\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "# Create output folder    \n",
    "output_path = os.path.join(directory, f\"output_{data_ver}_{Trials}-trials_{K}-fold_{Loss_name}\")\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                              II. K-fold Split  ~ IV. Model Training\n",
    "\n",
    "#%%\n",
    "\n",
    "# for K-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# for input data sequence generation before modeling\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for model fitting: generate data sequences    \n",
    "datagen = ImageDataGenerator(\n",
    "                                width_shift_range = 0.0,     # 水平平移\n",
    "                                height_shift_range = 0.0,    # 垂直平移\n",
    "                                rotation_range = 90,         # 0-180 任一角度旋轉\n",
    "                                horizontal_flip = True,      # 任意水平翻轉\n",
    "                                vertical_flip = True,        # 任意垂直翻轉\n",
    "                                fill_mode = \"constant\",      # 在旋轉或平移時，有空隙發生，則空隙補常數\n",
    "                                cval = 0                     # 設定常數值為 0\n",
    "                                )\n",
    "    \n",
    "#%%                                            II.2 Repetition (i.e., iteration of multiple trials)  \n",
    "#np.random.seed(214)       \n",
    "for trial in range(Trials):\n",
    "    trial = trial + 0 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    print(trial)\n",
    "    #Random Split\n",
    "    Val_split_data, Val_split_index = S1.Split_Data_Strafied(Val_path, Labels, Val_split_ratio, resize = resize) \n",
    "    Test_split_data, Test_split_index = S1.Split_Data_Strafied(Test_path, Labels, Test_split_ratio, resize = resize) \n",
    "\n",
    "    Val_split_data_INTER, Val_split_index_INTER = S1.Split_Data_Strafied(Val_INTER_path, Labels_INTER, Val_split_INTER_ratio, resize = resize) \n",
    "    Test_split_data_INTER, Test_split_index_INTER = S1.Split_Data_Strafied(Test_INTER_path, Labels_INTER, Test_split_INTER_ratio, resize = resize) \n",
    "       \n",
    "    #Extract and name corresponding data - hard label ----------------------------------------------------------\n",
    "    train_data = np.concatenate([split['train'] for label, split in Val_split_data.items()], 0)\n",
    "    train_label = np.concatenate([np.zeros(len(split['train']), dtype=np.int_) + Labels_dic[label] for label, split in Val_split_data.items()], 0).astype(np.int_)\n",
    "    train_index = [x  for label, split in Val_split_index.items() for x in split['train']]\n",
    "\n",
    "    test_data = np.concatenate([split['test'] for label, split in Test_split_data.items()], 0)\n",
    "    test_label = np.concatenate([np.zeros(len(split['test']), dtype=np.int_) + Labels_dic[label] for label, split in Test_split_data.items()], 0).astype(np.int_)\n",
    "    test_index = [x  for label, split in Test_split_index.items() for x in split['test']]\n",
    "    np.save(os.path.join(output_path, 'test_label.npy'), test_label)\n",
    "\n",
    "    test_data = S1.Resize_Prep(test_data, model_resize) # resize testing set at the beginning, for model input\n",
    "\n",
    "\n",
    "    #Extract and name corresponding data - soft label ------------------------------------------------------------    \n",
    "    train_data_INTER = np.concatenate([split['train'] for label, split in Val_split_data_INTER.items() ], 0)       \n",
    "    train_label_INTER = np.concatenate( [np.array([[1/3, 2/3, 0]]*len(Val_split_index_INTER[Labels_INTER[0]]['train'])),\n",
    "                                         np.array([[2/3, 1/3, 0]]*len(Val_split_index_INTER[Labels_INTER[1]]['train'])),\n",
    "                                         np.array([[1/4, 3/4, 0]]*len(Val_split_index_INTER[Labels_INTER[2]]['train'])),\n",
    "                                         np.array([[3/4, 1/4, 0]]*len(Val_split_index_INTER[Labels_INTER[3]]['train'])) ], axis=0\n",
    "                                        )\n",
    "    train_index_INTER = [x  for label, split in Val_split_index_INTER.items() for x in split['train']] #before x in split[] > if label not in [Labels_INTER[i]]\n",
    "\n",
    "    val_data_INTER = np.concatenate([split['val'] for label, split in Val_split_data_INTER.items() ], 0)\n",
    "    val_label_INTER = np.concatenate( [np.array([[1/3, 2/3, 0]]*len(Val_split_index_INTER[Labels_INTER[0]]['val'])),\n",
    "                                       np.array([[2/3, 1/3, 0]]*len(Val_split_index_INTER[Labels_INTER[1]]['val'])),\n",
    "                                       np.array([[1/4, 3/4, 0]]*len(Val_split_index_INTER[Labels_INTER[2]]['val'])),\n",
    "                                       np.array([[3/4, 1/4, 0]]*len(Val_split_index_INTER[Labels_INTER[3]]['val'])) ], axis=0\n",
    "                                        )\n",
    "    val_index_INTER = [x  for label, split in Val_split_index_INTER.items() for x in split['val']]\n",
    "\n",
    "    test_data_INTER = np.concatenate([split['test'] for label, split in Test_split_data_INTER.items() ], 0)\n",
    "    test_label_INTER = np.concatenate( [np.array([[1/3, 2/3, 0]]*len(Test_split_index_INTER[Labels_INTER[0]]['test'])),\n",
    "                                        np.array([[2/3, 1/3, 0]]*len(Test_split_index_INTER[Labels_INTER[1]]['test'])),\n",
    "                                        np.array([[1/4, 3/4, 0]]*len(Test_split_index_INTER[Labels_INTER[2]]['test'])),\n",
    "                                        np.array([[3/4, 1/4, 0]]*len(Test_split_index_INTER[Labels_INTER[3]]['test'])) ], axis=0\n",
    "                                        )\n",
    "    test_index_INTER = [x  for label, split in Test_split_index_INTER.items() for x in split['test']]\n",
    "    np.save(os.path.join(output_path, 'test_label_INTER.npy'), test_label_INTER)\n",
    "    \n",
    "    test_data_INTER = S1.Resize_Prep(test_data_INTER, model_resize) # resize testing set at the beginning, for model input\n",
    "    \n",
    "    #save sampel size\n",
    "    val_INTER_n = {}\n",
    "    for i in range(len(Val_split_index_INTER)):\n",
    "        val_INTER_n[i] = len(Val_split_index_INTER[Labels_INTER[i]]['val'])\n",
    "    test_INTER_n = {}\n",
    "    for i in range(len(Test_split_index_INTER)):\n",
    "        test_INTER_n[i] = len(Test_split_index_INTER[Labels_INTER[i]]['test'])\n",
    "\n",
    "        \n",
    "    \n",
    "    #save test indices for each trial ------------------------------------------------------------------------\n",
    "    with open(os.path.join(output_path, f'test_index_{Prep}-{other_info}_t{trial}.csv'), 'w') as temp_file:\n",
    "        for text in test_index:\n",
    "            temp_file.write(str(text)+'\\n')\n",
    "    with open(os.path.join(output_path, f'test_index_INTER_{Prep}-{other_info}_t{trial}.csv'), 'w') as temp_file:\n",
    "        for text in test_index_INTER:\n",
    "            temp_file.write(str(text)+'\\n')\n",
    "                \n",
    "    del Val_split_data, Val_split_index, Test_split_data, Test_split_index, Val_split_data_INTER, Val_split_index_INTER, Test_split_data_INTER, Test_split_index_INTER\n",
    "        \n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "\n",
    "        #%%                                            II.2 K-fold Split\n",
    "            \n",
    "    for k, (train_skf_indices, val_indices) in enumerate(skf.split(train_data, train_label)):\n",
    "        print(f\"\\n Current trial:\\t \" + str(trial+1) + f\"/{Trials}...\")\n",
    "        print(f\"\\n Training on fold \" + str(k+1) + f\"/{K}...\")\n",
    "            #print(\"TRAIN:\", train_skf_indices, \"\\n TEST:\", val_indices)\n",
    "            \n",
    "        \n",
    "        # Preprocess (if any) to k-fold training dataset\n",
    "        if Prep == None: # train_skf_data + INTER data\n",
    "            print(f\"Training set preprocessing: {Prep}\")\n",
    "                \n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            N_exp = 0\n",
    "            N_exp2 = 0\n",
    "            N_exp3 = 0\n",
    "            N_exp4 = 0                 \n",
    "        \n",
    "        if Prep == \"INTER\": # train_skf_data + INTER data\n",
    "            print(f\"Training set preprocessing: {Prep}\")\n",
    "                \n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "                \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "                \n",
    "            # Concatenate original and INTER data\n",
    "            train_skf_data =  np.concatenate([train_skf_data, train_skf_data_INTER], 0)# ready for model input\n",
    "            train_skf_label = np.concatenate([train_skf_label, train_label_INTER], 0) # one-hot encoder \n",
    "            \n",
    "            N_exp = 0\n",
    "            N_exp2 = 0\n",
    "            N_exp3 = 0\n",
    "            N_exp4 = 0\n",
    "\n",
    "            del train_skf_data_INTER\n",
    "\n",
    "\n",
    "        if Prep == \"OS_INTER\": # train_skf_data + INTER data\n",
    "            print(f\"Training set preprocessing: {Prep}\")\n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "            \n",
    "            #prepare data for OS ---------------------------------------------------------------------------------------------                          \n",
    "            #reample label_lists[i] with size = N_exp_list[i]\n",
    "            N_exp  = 128*2\n",
    "            N_exp2 = 128*2\n",
    "            N_exp3 = 128\n",
    "            N_exp4 = 128*2\n",
    "\n",
    "            N_exp_list = [N_exp, N_exp2, N_exp3, N_exp4]\n",
    "            label_lists = [[3/4, 1/4, 0], [1/4, 3/4, 0], [2/3, 1/3, 0], [1/3, 2/3, 0]]\n",
    "            \n",
    "            data_OS = np.zeros((0,resize[0],resize[1],3))\n",
    "            label_OS_m = np.zeros((0, len(Labels)))\n",
    "            for N_exp0, label_list in zip(N_exp_list, label_lists):\n",
    "                #print(n,'\\t', label_list)\n",
    "                data_OS0, label_OS_m0 = S2.Select_Resample(train_skf_data_INTER, train_label_INTER, label_list, N_exp0)                \n",
    "                data_OS = np.concatenate([data_OS, data_OS0], 0)\n",
    "                label_OS_m = np.concatenate([label_OS_m, label_OS_m0])\n",
    "                \n",
    "            # Concatenate original,INTER and SMOTE data --------------------------------------------------------------\n",
    "            train_skf_data =  np.concatenate([train_skf_data, train_skf_data_INTER, data_OS], 0)# ready for model input\n",
    "            train_skf_label = np.concatenate([train_skf_label, train_label_INTER, label_OS_m], 0) # one-hot encoder \n",
    "            del train_skf_data_INTER, data_OS, label_OS_m\n",
    "\n",
    "        if Prep == \"DS_INTER\": # train_skf_data + INTER data \n",
    "            print(f\"Training set preprocessing: {Prep}\")\n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "            \n",
    "            img_shape = train_skf_data.shape[1:]\n",
    "            \n",
    "            ## Operation (1): Down-Sample [2/3, 1/3, 0] V  vs [1/3, 2/3, 0] V\n",
    "            data_list = [train_skf_data_INTER, train_skf_data_INTER]\n",
    "            target_list = [train_label_INTER, train_label_INTER]\n",
    "            label_lists = [[2/3, 1/3, 0], [1/3, 2/3, 0]] # output labels later will be [0, 1] correspondingly                 \n",
    "            #DS preprocessing            \n",
    "            data_concat1, target_concat_m1, N_concat = S2.DS_Prep(data_list, target_list, label_lists, img_shape=img_shape, sampling_strategy=\"auto\")\n",
    "            N210 = N_concat[0]; N120 = N_concat[1]\n",
    "\n",
    "                        \n",
    "            ## Operation (2): Down-Sample [2/3, 1/3, 0] V  vs [3/4, 1/4, 0] V\n",
    "            data_list = [train_skf_data_INTER, train_skf_data_INTER]\n",
    "            target_list = [train_label_INTER, train_label_INTER]\n",
    "            label_lists = [[2/3, 1/3, 0], [3/4, 1/4, 0]] # output labels later will be [0, 1] correspondingly                 \n",
    "            #DS preprocessing            \n",
    "            data_concat2, target_concat_m2, N_concat = S2.DS_Prep(data_list, target_list, label_lists, img_shape=img_shape, sampling_strategy=\"all\")\n",
    "\n",
    "            data_concat2 = S2.Select_subset(data_concat2, target_concat_m2, label_lists[1])\n",
    "            target_concat_m2 = np.array(label_lists[1]*N_concat[1]).reshape((-1, len(Labels)))\n",
    "            N310 = N_concat[1]\n",
    "\n",
    "            \n",
    "            ## Operation (3): Down-Sample [4/5, 1/5, 0]  vs [3/4, 1/4, 0] V\n",
    "            data_list = [train_skf_data_INTER, train_skf_data_INTER]\n",
    "            target_list = [train_label_INTER, train_label_INTER]\n",
    "            label_lists = [[1/3, 2/3, 0], [1/4, 3/4, 0]] # output labels later will be [0, 1] correspondingly                 \n",
    "            #DS preprocessing            \n",
    "            data_concat3, target_concat_m3, N_concat = S2.DS_Prep(data_list, target_list, label_lists, img_shape=img_shape, sampling_strategy=\"all\")\n",
    "\n",
    "            data_concat3 = S2.Select_subset(data_concat3, target_concat_m3, label_lists[1])\n",
    "            target_concat_m3 = np.array(label_lists[1]*N_concat[1]).reshape((-1, len(Labels)))\n",
    "            N130 = N_concat[1]\n",
    "             \n",
    "            \n",
    "            ## Operation (0): Selet Data without Down-Sampling [1,0,0] vs [0,1,0] vs [0,0,1]\n",
    "            data_list = [train_skf_data, train_skf_data, train_skf_data]\n",
    "            target_list = [train_skf_label, train_skf_label, train_skf_label]\n",
    "            label_lists = [[1,0,0], [0,1,0], [0,0,1]] # output labels later will be [0, 1, 2] correspondingly                 \n",
    "                \n",
    "            data_concat_flat, target_concat, N_concat = S2.Select_subset_flatten(data_list, target_list, label_lists )\n",
    "            data_concat0, target_concat_m0 = S2.ReshapeX_TransY(data_concat_flat, target_concat, label_lists, img_shape=img_shape)\n",
    "\n",
    "            DS_N = pd.DataFrame([OrderedDict({'N310': N310, 'N130': N130, 'N210': N210, 'N120': N120})])           \n",
    "            DS_N[\"K-fold\"] = k\n",
    "            DS_N[\"Trial\"] = trial\n",
    "            \n",
    "            \n",
    "            if other_info == \"OS\":\n",
    "                print(f\"Plus training set preprocessing: {Prep}\")\n",
    "                \n",
    "                train_skf_data = data_concat0 # ready for model input\n",
    "                train_skf_label = target_concat_m0 # one-hot encoder\n",
    "            \n",
    "                train_skf_data_INTER_s = np.concatenate([data_concat1, data_concat2, data_concat3], 0)# ready for model input\n",
    "                train_label_INTER_s = np.concatenate([target_concat_m1, target_concat_m2, target_concat_m3], 0) # one-hot encoder \n",
    "            \n",
    "                #prepare data for OS ---------------------------------------------------------------------------------------------                          \n",
    "                #reample label_lists[i] with size = N_exp_list[i]\n",
    "                N_exp  = 128*2\n",
    "                N_exp2 = 128*2\n",
    "                N_exp3 = 128\n",
    "                N_exp4 = 128*2\n",
    "                N_exp_list = [N_exp, N_exp2, N_exp3, N_exp4]\n",
    "                label_lists = [[3/4, 1/4, 0], [1/4, 3/4, 0], [2/3, 1/3, 0], [1/3, 2/3, 0]]\n",
    "                \n",
    "                data_OS = np.zeros((0,resize[0],resize[1],3))\n",
    "                label_OS_m = np.zeros((0, len(Labels)))\n",
    "                for N_exp0, label_list in zip(N_exp_list, label_lists):\n",
    "                    #print(n,'\\t', label_list)\n",
    "                    data_OS0, label_OS_m0 = S2.Select_Resample(train_skf_data_INTER_s, train_label_INTER_s, label_list, N_exp0)                \n",
    "                    data_OS = np.concatenate([data_OS, data_OS0], 0)\n",
    "                    label_OS_m = np.concatenate([label_OS_m, label_OS_m0])\n",
    "                \n",
    "                # Concatenate original,INTER and SMOTE data --------------------------------------------------------------\n",
    "                train_skf_data =  np.concatenate([train_skf_data, train_skf_data_INTER_s, data_OS], 0)# ready for model input\n",
    "                train_skf_label = np.concatenate([train_skf_label, train_label_INTER_s, label_OS_m], 0) # one-hot encoder \n",
    "                del train_skf_data_INTER_s,train_label_INTER_s, data_OS, label_OS_m\n",
    "\n",
    "            elif other_info == None:\n",
    "                N_exp, N_exp2, N_exp3, N_exp4  = None, None, None, None\n",
    "                # Concatenate original,INTER and SMOTE data --------------------------------------------------------------\n",
    "                train_skf_data =  np.concatenate([data_concat0, data_concat1, data_concat2, data_concat3], 0)# ready for model input\n",
    "                train_skf_label = np.concatenate([target_concat_m0, target_concat_m1, target_concat_m2, target_concat_m3], 0) # one-hot encoder \n",
    "\n",
    "\n",
    "            del data_concat0, data_concat1, data_concat2, data_concat3\n",
    "            del target_concat_m0, target_concat_m1, target_concat_m2, target_concat_m3\n",
    "            del data_list, target_list, label_lists, N_concat\n",
    "\n",
    "        \n",
    "        if Prep == \"Bord_SMOTE_INTER\": # train_skf_data + INTER data \n",
    "            print(f\"Training set preprocessing: {Prep}\")\n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "\n",
    "            #prepare data for SMOTE (with Mixeup)---------------------------------------------------------------------------------------------                          \n",
    "            ## Case (1): [1,0,0] vs [3/4, 1/4, 0]\n",
    "            N_exp  = 256\n",
    "            label_lists = [[1,0,0], [3/4, 1/4, 0]] \n",
    "              \n",
    "            data_SMOTE01 = S2.Select_subset(train_skf_data_INTER, train_label_INTER, label_list=label_lists[1]) \n",
    "            data_SMOTE00, _ = S2.Select_Resample(train_skf_data, train_skf_label, label_list=label_lists[0], N_exp=N_exp+len(data_SMOTE01)) #S2.Select_Resample(train_skf_data, train_skf_label, label_list=label_lists[0], N_exp=N_exp)        \n",
    "            data_SMOTE0 = np.concatenate([data_SMOTE00, data_SMOTE01 ]) #data_SMOTE02\n",
    "            #transform the dataset\n",
    "            data_SMOTE0_flat = data_SMOTE0.reshape((len(data_SMOTE0), -1))\n",
    "            label_SMOTE0_t = np.concatenate([ np.array([0]*len(data_SMOTE00)), np.array([1]*len(data_SMOTE01)) ])\n",
    "            print(Counter(label_SMOTE0_t))\n",
    "            oversampler = S2.BorderlineSMOTE(sampling_strategy=\"auto\", kind='borderline-1')                #{1: N_exp + len(data_SMOTE01)}\n",
    "            data_SMOTE0_flat2, label_SMOTE0_t2 = oversampler.fit_resample(data_SMOTE0_flat, label_SMOTE0_t)\n",
    "            print(Counter(label_SMOTE0_t2))\n",
    "            \n",
    "            data_SMOTE0_flat2 = data_SMOTE0_flat2[label_SMOTE0_t2==1]\n",
    "            data_SMOTE = data_SMOTE0_flat2.reshape(data_SMOTE00.shape)\n",
    "            label_SMOTE_m = np.array([label_lists[1]*(N_exp+len(data_SMOTE01))]).reshape((-1, 3))                            \n",
    "            del data_SMOTE00, data_SMOTE01, data_SMOTE0, data_SMOTE0_flat, data_SMOTE0_flat2, label_SMOTE0_t, label_SMOTE0_t2\n",
    "\n",
    "            ## Case (2): [0,1,0] vs [1/4, 3/4, 0]\n",
    "            N_exp2  = 256\n",
    "            label_lists = [[0,1,0] , [1/4, 3/4, 0]] \n",
    "              \n",
    "            data_SMOTE01 = S2.Select_subset(train_skf_data_INTER, train_label_INTER, label_list=label_lists[1])            \n",
    "            data_SMOTE00, _ = S2.Select_Resample(train_skf_data, train_skf_label, label_list=label_lists[0], N_exp=N_exp2+len(data_SMOTE01)) #S2.Select_Resample(train_skf_data, train_skf_label, label_list=label_lists[0], N_exp=N_exp2) #\n",
    "            data_SMOTE0 = np.concatenate([data_SMOTE00, data_SMOTE01 ]) #data_SMOTE02\n",
    "            #transform the dataset\n",
    "            data_SMOTE0_flat = data_SMOTE0.reshape((len(data_SMOTE0), -1))\n",
    "            label_SMOTE0_t = np.concatenate([ np.array([0]*len(data_SMOTE00)), np.array([1]*len(data_SMOTE01)) ])\n",
    "            print(Counter(label_SMOTE0_t))\n",
    "            oversampler = S2.BorderlineSMOTE(sampling_strategy=\"auto\", kind='borderline-1')                #{1: N_exp + len(data_SMOTE01)}\n",
    "            data_SMOTE0_flat2, label_SMOTE0_t2 = oversampler.fit_resample(data_SMOTE0_flat, label_SMOTE0_t)\n",
    "            print(Counter(label_SMOTE0_t2))\n",
    "                            \n",
    "            data_SMOTE0_flat2 = data_SMOTE0_flat2[label_SMOTE0_t2==1]\n",
    "            data_SMOTE2 = data_SMOTE0_flat2.reshape(data_SMOTE00.shape)\n",
    "            label_SMOTE_m2 = np.array([label_lists[1]*(N_exp2+len(data_SMOTE01))]).reshape((-1, 3))                            \n",
    "            del data_SMOTE00, data_SMOTE01, data_SMOTE0, data_SMOTE0_flat, data_SMOTE0_flat2, label_SMOTE0_t, label_SMOTE0_t2\n",
    "\n",
    "            ## Case (3): [3/4, 1/4, 0] vs [2/3, 1/3, 0]\n",
    "            N_exp3 = 128\n",
    "            label_lists = [[1,0,0], [2/3, 1/3, 0]] \n",
    "              \n",
    "            data_SMOTE01 = S2.Select_subset(train_skf_data_INTER, train_label_INTER, label_list=label_lists[1])\n",
    "            data_SMOTE00, _ = S2.Select_Resample(train_skf_data, train_skf_label, label_list=label_lists[0], N_exp=N_exp3+len(data_SMOTE01))\n",
    "            data_SMOTE0 = np.concatenate([data_SMOTE00, data_SMOTE01 ]) #data_SMOTE02\n",
    "            #transform the dataset\n",
    "            data_SMOTE0_flat = data_SMOTE0.reshape((len(data_SMOTE0), -1))\n",
    "            label_SMOTE0_t = np.concatenate([ np.array([0]*len(data_SMOTE00)), np.array([1]*len(data_SMOTE01)) ])\n",
    "            print(Counter(label_SMOTE0_t))\n",
    "            oversampler = S2.BorderlineSMOTE(sampling_strategy='auto', kind='borderline-1')                #{1: N_exp + len(data_SMOTE01)}\n",
    "            data_SMOTE0_flat2, label_SMOTE0_t2 = oversampler.fit_resample(data_SMOTE0_flat, label_SMOTE0_t)\n",
    "            print(Counter(label_SMOTE0_t2))\n",
    "                            \n",
    "            data_SMOTE0_flat2 = data_SMOTE0_flat2[label_SMOTE0_t2==1]\n",
    "            data_SMOTE3 = data_SMOTE0_flat2.reshape(data_SMOTE00.shape)\n",
    "            label_SMOTE_m3 = np.array([label_lists[1]*(N_exp3+len(data_SMOTE01))]).reshape((-1, 3))                            \n",
    "            del data_SMOTE00, data_SMOTE01, data_SMOTE0, data_SMOTE0_flat, data_SMOTE0_flat2, label_SMOTE0_t, label_SMOTE0_t2\n",
    "\n",
    "\n",
    "            ## Case (4): [1/4, 3/4, 0] vs [1/3, 2/3, 0]\n",
    "            N_exp4 = 256\n",
    "            label_lists = [[1/4, 3/4, 0], [1/3, 2/3, 0]] \n",
    "            \n",
    "            data_SMOTE01 = S2.Select_subset(train_skf_data_INTER, train_label_INTER, label_list=label_lists[1])\n",
    "            data_SMOTE00, _ = S2.Select_Resample(train_skf_data_INTER, train_label_INTER, label_list=label_lists[0], N_exp=N_exp4+len(data_SMOTE01))\n",
    "            data_SMOTE0 = np.concatenate([data_SMOTE00, data_SMOTE01 ]) #data_SMOTE02\n",
    "            #transform the dataset\n",
    "            data_SMOTE0_flat = data_SMOTE0.reshape((len(data_SMOTE0), -1))\n",
    "            label_SMOTE0_t = np.concatenate([ np.array([0]*len(data_SMOTE00)), np.array([1]*len(data_SMOTE01)) ])\n",
    "            print(Counter(label_SMOTE0_t))\n",
    "            oversampler = S2.BorderlineSMOTE(sampling_strategy='auto', kind='borderline-1')                #{1: N_exp + len(data_SMOTE01)}\n",
    "            data_SMOTE0_flat2, label_SMOTE0_t2 = oversampler.fit_resample(data_SMOTE0_flat, label_SMOTE0_t)\n",
    "            print(Counter(label_SMOTE0_t2))\n",
    "\n",
    "            data_SMOTE0_flat2 = data_SMOTE0_flat2[label_SMOTE0_t2==1]\n",
    "            data_SMOTE4 = data_SMOTE0_flat2.reshape(data_SMOTE00.shape)\n",
    "            label_SMOTE_m4 = np.array([label_lists[1]*(N_exp4+len(data_SMOTE01))]).reshape((-1, 3))                            \n",
    "            del data_SMOTE00, data_SMOTE01, data_SMOTE0, data_SMOTE0_flat, data_SMOTE0_flat2, label_SMOTE0_t, label_SMOTE0_t2\n",
    "                            \n",
    "\n",
    "            # Concatenate original,INTER and SMOTE data --------------------------------------------------------------\n",
    "            train_skf_data =  np.concatenate([train_skf_data, data_SMOTE, data_SMOTE2, data_SMOTE3, data_SMOTE4], 0)# ready for model input\n",
    "            train_skf_label = np.concatenate([train_skf_label, label_SMOTE_m, label_SMOTE_m2, label_SMOTE_m3, label_SMOTE_m4], 0) # one-hot encoder \n",
    "            del train_skf_data_INTER, data_SMOTE, data_SMOTE2, data_SMOTE3, data_SMOTE4\n",
    "            del label_SMOTE_m, label_SMOTE_m2, label_SMOTE_m3, label_SMOTE_m4\n",
    "\n",
    "\n",
    "        \"\"\"  \n",
    "        if Prep == \"SMOTE_INTER\":\n",
    "            print(f\"Training set preprocessing: {Prep}{other_info}\")\n",
    "\n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "            \n",
    "            #prepare data for SMOTE (with Mixeup)---------------------------------------------------------------------------------------------                          \n",
    "            ## Case (1): [1,0,0] vs [4/5,1/5,0]\n",
    "            N_exp = 849\n",
    "            if N_exp >0:\n",
    "                data_SMOTE, label_SMOTE = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=0, label_val=[1, 4/5], Resample= False)\n",
    "                data_SMOTE, label_SMOTE = S2.SMOTE_SoftLabel(data_SMOTE, label_SMOTE, state=SMOTE_state, n_sample=N_exp)\n",
    "                label_SMOTE_m = np.zeros((len(label_SMOTE),len(Labels)))\n",
    "                label_SMOTE_m[:,0] = label_SMOTE #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m[:,1] = 1- label_SMOTE\n",
    "                label_SMOTE_m[:,2] = 0\n",
    "                del label_SMOTE\n",
    "            else:\n",
    "                data_SMOTE = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m = np.zeros((0, 3))\n",
    "            #output: [data_SMOTE, label_SMOTE_m]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_410.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''            \n",
    "\n",
    "            ## Case (2): [0,1,0] vs [1/5,4/5,0]\n",
    "            N_exp2 = 0\n",
    "            if N_exp2 >0:\n",
    "                data_SMOTE2, label_SMOTE2 = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=1, label_val=[1, 4/5], Resample= False)\n",
    "                data_SMOTE2, label_SMOTE2 = S2.SMOTE_SoftLabel(data_SMOTE2, label_SMOTE2, state=SMOTE_state, n_sample=N_exp2)\n",
    "                label_SMOTE_m2 = np.zeros((len(label_SMOTE2),len(Labels)))\n",
    "                label_SMOTE_m2[:,0] = 1- label_SMOTE2 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m2[:,1] = label_SMOTE2\n",
    "                label_SMOTE_m2[:,2] = 0\n",
    "                del label_SMOTE2\n",
    "            else:\n",
    "                data_SMOTE2 = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m2 = np.zeros((0, 3))\n",
    "            #output: [data_SMOTE2, label_SMOTE_m2]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE2)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_140.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "\n",
    "            ## Case (3): [4/5,1/5,0] vs [3/4, 1/4, 0]\n",
    "            N_exp3 = 0\n",
    "            if N_exp3 >0:\n",
    "                data_SMOTE3, label_SMOTE3 = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=0, label_val=[4/5, 3/4], Resample= False)\n",
    "                data_SMOTE3, label_SMOTE3 = S2.SMOTE_SoftLabel(data_SMOTE3, label_SMOTE3, state=SMOTE_state, n_sample=N_exp3)\n",
    "                label_SMOTE_m3 = np.zeros((len(label_SMOTE3),len(Labels)))\n",
    "                label_SMOTE_m3[:,0] = label_SMOTE3 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m3[:,1] = 1- label_SMOTE3\n",
    "                label_SMOTE_m3[:,2] = 0\n",
    "                del label_SMOTE3\n",
    "            else:\n",
    "                data_SMOTE3 = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m3 = np.zeros((0, 3))                \n",
    "            #output: [data_SMOTE3, label_SMOTE_m3]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE3)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_310.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "\n",
    "            ## Case (4): [1/5,4/5,0] vs [1/4, 3/4, 0]\n",
    "            N_exp4 = 0\n",
    "            if N_exp4 >0:\n",
    "                data_SMOTE4, label_SMOTE4 = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=1, label_val=[4/5, 3/4], Resample= False)\n",
    "                data_SMOTE4, label_SMOTE4 = S2.SMOTE_SoftLabel(data_SMOTE4, label_SMOTE4, state=SMOTE_state, n_sample=N_exp4)\n",
    "                label_SMOTE_m4 = np.zeros((len(label_SMOTE4),len(Labels)))\n",
    "                label_SMOTE_m4[:,0] = 1- label_SMOTE4 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m4[:,1] = label_SMOTE4\n",
    "                label_SMOTE_m4[:,2] = 0\n",
    "                del label_SMOTE4\n",
    "            else:\n",
    "                data_SMOTE4 = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m4 = np.zeros((0, 3))\n",
    "            #output: [data_SMOTE4, label_SMOTE_m4]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE4)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_130.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            ## Case (5): [3/4, 1/4, 0] vs [2/3, 1/3, 0]\n",
    "            N_exp5 = 496\n",
    "            if N_exp5 >0:\n",
    "                data_SMOTE5, label_SMOTE5 = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=0, label_val=[3/4 ,2/3], Resample= False)\n",
    "                data_SMOTE5, label_SMOTE5 = S2.SMOTE_SoftLabel(data_SMOTE5, label_SMOTE5, state=SMOTE_state, n_sample=N_exp5)\n",
    "                label_SMOTE_m5 = np.zeros((len(label_SMOTE5),len(Labels)))\n",
    "                label_SMOTE_m5[:,0] = label_SMOTE5 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m5[:,1] = 1- label_SMOTE5\n",
    "                label_SMOTE_m5[:,2] = 0\n",
    "                del label_SMOTE5\n",
    "            else:\n",
    "                data_SMOTE5 = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m5 = np.zeros((0, 3))            \n",
    "            #output: [data_SMOTE5, label_SMOTE_m5]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE5)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_210.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "\n",
    "            ## Case (6): [1/4, 3/4, 0] vs [1/3, 2/3, 0]\n",
    "            N_exp6 = 439\n",
    "            if N_exp6 >0:\n",
    "                data_SMOTE6, label_SMOTE6 = select_data2(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=1, label_val=[3/4 ,2/3], Resample= False)\n",
    "                data_SMOTE6, label_SMOTE6 = S2.SMOTE_SoftLabel(data_SMOTE6, label_SMOTE6, state=SMOTE_state, n_sample=N_exp6)\n",
    "                label_SMOTE_m6 = np.zeros((len(label_SMOTE6),len(Labels)))\n",
    "                label_SMOTE_m6[:,0] = 1- label_SMOTE6 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "                label_SMOTE_m6[:,1] = label_SMOTE6\n",
    "                label_SMOTE_m6[:,2] = 0\n",
    "                del label_SMOTE6\n",
    "            else:\n",
    "                data_SMOTE6 = np.zeros((0,300,300,3))\n",
    "                label_SMOTE_m6 = np.zeros((0, 3))\n",
    "            #output: [data_SMOTE6, label_SMOTE_m6]\n",
    "            '''\n",
    "            plt.hist(label_SMOTE6)\n",
    "            plt.savefig(f\"Histogram of SMOTEd labels_state{SMOTE_state}_120.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            # Concatenate original,INTER and SMOTE data --------------------------------------------------------------\n",
    "            train_skf_data =  np.concatenate([train_skf_data, train_skf_data_INTER, data_SMOTE, data_SMOTE2, data_SMOTE3, data_SMOTE4, data_SMOTE5, data_SMOTE6], 0)# ready for model input\n",
    "            train_skf_label = np.concatenate([train_skf_label, train_label_INTER, label_SMOTE_m, label_SMOTE_m2, label_SMOTE_m3, label_SMOTE_m4, label_SMOTE_m5, label_SMOTE_m6], 0) # one-hot encoder \n",
    "            del train_skf_data_INTER, data_SMOTE, data_SMOTE2, data_SMOTE3, data_SMOTE4, data_SMOTE5, data_SMOTE6\n",
    "            del label_SMOTE_m, label_SMOTE_m2, label_SMOTE_m3, label_SMOTE_m4, label_SMOTE_m5, label_SMOTE_m6\n",
    "\n",
    "\n",
    "        if Prep == \"CutMix_INTER\":\n",
    "            print(f\"Training set preprocessing: {Prep}{other_info}\")\n",
    "\n",
    "            train_skf_data = S1.Resize_Prep(train_data[train_skf_indices], model_resize) # ready for model input\n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "            \n",
    "            train_skf_data_INTER = S1.Resize_Prep(train_data_INTER, model_resize) \n",
    "            \n",
    "            #prepare data for CutMix----------------------------------------------------------------------------------------------  \n",
    "            ## Case (0): [1,0,0] vs [2/3, 1/3, 0]\n",
    "            N_exp = 128\n",
    "            data_CutMix, label_CutMix = select_data(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=0, N_exp= N_exp, Resample= True)\n",
    "            data_CutMix, label_CutMix = S2.CutMix_Prep_gen(data_CutMix, label_CutMix, state=CutMix_state, model_resize=model_resize) # ready for model input            \n",
    "            label_CutMix_m = np.zeros((len(label_CutMix),len(Labels)))\n",
    "            label_CutMix_m[:,0] = label_CutMix #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            label_CutMix_m[:,1] = 1- label_CutMix\n",
    "            label_CutMix_m[:,2] = 0\n",
    "            #output: [data_CutMix, label_CutMix_m]\n",
    "            '''\n",
    "            plt.hist(label_CutMix)\n",
    "            plt.savefig(f\"Histogram of CutMixed labels_state{CutMix_state}_210.png\", transparent=True)\n",
    "            plt.close()\n",
    "            '''\n",
    "            del label_CutMix\n",
    "\n",
    "            ## Case (1): [0,1,0] vs [1/3, 2/3, 0]\n",
    "            N_exp2 = 128\n",
    "            data_CutMix2, label_CutMix2 = select_data(train_skf_data, train_skf_label, train_skf_data_INTER, train_label_INTER, label=1, N_exp= N_exp2, Resample= True)            \n",
    "            data_CutMix2, label_CutMix2 = S2.CutMix_Prep_gen(data_CutMix2, label_CutMix2, state=CutMix_state, model_resize=model_resize) # ready for model input\n",
    "            label_CutMix_m2 = np.zeros((len(label_CutMix2),len(Labels)))\n",
    "            label_CutMix_m2[:,0] = 1- label_CutMix2 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            label_CutMix_m2[:,1] = label_CutMix2\n",
    "            label_CutMix_m2[:,2] = 0\n",
    "            #output: [data_CutMix2, label_CutMix_m2]\n",
    "            del label_CutMix2\n",
    "           \n",
    "            # Concatenate original,INTER and CutMix data --------------------------------------------------------------\n",
    "            train_skf_data =  np.concatenate([train_skf_data, train_skf_data_INTER, data_CutMix, data_CutMix2], 0)# ready for model input\n",
    "            train_skf_label = np.concatenate([train_skf_label, train_label_INTER, label_CutMix_m, label_CutMix_m2], 0) # one-hot encoder \n",
    "            del train_skf_data_INTER, data_CutMix, data_CutMix2, label_CutMix_m, label_CutMix_m2\n",
    "    \n",
    "        if Prep == \"Puzzle\":\n",
    "            print(f\"Training set preprocessing: {Prep}{other_info}\")\n",
    "            train_skf_data = train_data[train_skf_indices]\n",
    "            train_skf_data = S2.Puzzle_Prep(train_skf_data, len_crop, model_resize)     # ready for model input          \n",
    "            train_skf_label = np.eye(len(Labels))[train_label[train_skf_indices]] # one-hot encoder\n",
    "\n",
    "        \"\"\" \n",
    "        #%%                                                 II.1 Parameters for Loss Function\n",
    "        for alpha_list, gamma in zip(alpha_lists, gammas):\n",
    "            print(f\"\\n\\n alpha_list = {alpha_list}\\t gamma={gamma}\")\n",
    "            \n",
    "            #%%                                                IV. Model Training\n",
    "           \n",
    "            # call model\n",
    "            model = S3.create_EfficientNetB3(n_class=len(Labels), Loss=Loss_name, Activation =Activation)\n",
    "            print(f\"\\n Current parameters for loss function successful embedded - {Loss_name}: gamma={gamma}, alpha_list={alpha_list}\\n\")\n",
    "            \n",
    "            # Preprocess (if any) to k-fold validation dataset\n",
    "            val_skf_data = S1.Resize_Prep(train_data[val_indices], model_resize) # ready for model input \n",
    "            val_skf_label = np.eye(len(Labels))[train_label[val_indices]] \n",
    "    \n",
    "            # for model fitting: training with data sequences\n",
    "            callbacks_list = S3.call_LearningRateScheduler(S3.step_decay, verbose=1)\n",
    "            model_history = model.fit_generator(\n",
    "                                                datagen.flow(train_skf_data, train_skf_label, batch_size = batch_size),\n",
    "                                                steps_per_epoch = len(train_skf_indices) / batch_size,\n",
    "                                                validation_data = (val_skf_data, val_skf_label),\n",
    "                                                epochs = epochs,\n",
    "                                                verbose = 1,\n",
    "                                                callbacks = callbacks_list\n",
    "                                                )\n",
    "        \n",
    "            # Save model\n",
    "            model.save(os.path.join(output_path, f'EfficientNetB3_{Prep}{other_info}_t{trial}_K{k}.h5'))\n",
    "\n",
    "\n",
    "            #%%                                      V. Evaluation phases   \n",
    "            #%%\n",
    "        \n",
    "            #%%                                      V.1 Record History         \n",
    "            # training process\n",
    "            train_history = pd.DataFrame(model_history.history)\n",
    "            train_history[\"epoch\"] = list(range(epochs))\n",
    "            train_history[\"K-fold\"] = k \n",
    "            train_history[\"Trial\"] = trial\n",
    "            train_history[\"Focal_gamma\"] = gamma\n",
    "            train_history[\"Focal_alpha0\"] = alpha_list[0]\n",
    "            train_history[\"Focal_alpha1\"] = alpha_list[1]\n",
    "            train_history[\"N1\"] = N_exp\n",
    "            train_history[\"N2\"] = N_exp2\n",
    "            train_history[\"N3\"] = N_exp3\n",
    "            train_history[\"N4\"] = N_exp4       \n",
    "            #%%                                      V.2 Evaluation for metrics                  \n",
    "            # confution matrix and Accuracy, sensitivity, specificity\n",
    "            ## for  validation set of k-fold\n",
    "            val_predict_prob = model.predict(val_skf_data)\n",
    "            val_predict_label = np.argmax(val_predict_prob,axis=1) \n",
    "            if Activation == 'sigmoid':\n",
    "                ### Row normalization (Prob sum up t0 1)\n",
    "                val_predict_prob = val_predict_prob/val_predict_prob.sum(axis=1)[:, np.newaxis]\n",
    "   \n",
    "            report_val, overall_val = S4.Multiclass_metrics_OVR(y_pred_prob = val_predict_prob, y_pred_label = val_predict_label, \n",
    "                                                                y_true = train_label[val_indices], y_index_names = np.array(train_index)[val_indices], target_names = Labels)\n",
    "            report_val['dataset'] = 'val'\n",
    "            overall_val['dataset'] = 'val'\n",
    "    \n",
    "            ## for testing set\n",
    "            test_predict_prob = model.predict(test_data)\n",
    "            test_predict_label = np.argmax(test_predict_prob,axis=1)\n",
    "            if Activation == 'sigmoid':\n",
    "                ### Row normalization (Prob sum up t0 1)\n",
    "                test_predict_prob = test_predict_prob/test_predict_prob.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            report_test, overall_test = S4.Multiclass_metrics_OVR(y_pred_prob = test_predict_prob, y_pred_label = test_predict_label, \n",
    "                                                                  y_true = test_label, y_index_names = test_index, target_names = Labels)\n",
    "            report_test['dataset'] = 'test'\n",
    "            overall_test['dataset'] = 'test'\n",
    "\n",
    "\n",
    "            # concate results\n",
    "            report = pd.concat([report_val, report_test], axis =0)\n",
    "            report[\"Focal_gamma\"] = gamma\n",
    "            report[\"Focal_alpha0\"] = alpha_list[0]\n",
    "            report[\"Focal_alpha1\"] = alpha_list[1]\n",
    "            report[\"N1\"] = N_exp\n",
    "            report[\"N2\"] = N_exp2\n",
    "            report[\"N3\"] = N_exp3\n",
    "            report[\"N4\"] = N_exp4\n",
    "            report[\"Trial\"] = trial\n",
    "            report[\"K-fold\"] = k\n",
    "            \n",
    "            overall = pd.DataFrame([overall_val, overall_test])\n",
    "            overall[\"Focal_gamma\"] = gamma\n",
    "            overall[\"Focal_alpha0\"] = alpha_list[0]\n",
    "            overall[\"Focal_alpha1\"] = alpha_list[1]\n",
    "            overall[\"N1\"] = N_exp\n",
    "            overall[\"N2\"] = N_exp2\n",
    "            overall[\"N3\"] = N_exp3\n",
    "            overall[\"N4\"] = N_exp4\n",
    "            overall[\"Trial\"] = trial\n",
    "            overall[\"K-fold\"] = k            \n",
    "            \n",
    "            ## save testing set prediction (of each fold of model) -------------------------------------------\n",
    "            test_predict_prob_df = pd.DataFrame(test_predict_prob, columns=Labels)\n",
    "            test_predict_prob_df[\"accuracy\"] = overall_val['accuracy']\n",
    "            test_predict_prob_df[\"f1-score_macro\"] = overall_val['f1-score_macro']\n",
    "            test_predict_prob_df[\"Trial\"] = trial\n",
    "            test_predict_prob_df[\"K-fold\"] = k\n",
    "            \n",
    "            \n",
    "            del report_val, report_test\n",
    "            \n",
    "\n",
    "            \"\"\"        \n",
    "            #%%                                  V.3 Evaluation for ROC-AUC    \n",
    "            ##　for  validation set of k-fold\n",
    "            fpr, tpr, thresholds, roc_auc = S4.ROC_AUC(train_label[val_indices], val_predict_prob)      \n",
    "            val_roc = OrderedDict({\"fpr\": fpr[1], \"tpr\": tpr[1], \"thresholds\": thresholds[1]})\n",
    "            val_roc = pd.DataFrame.from_dict(val_roc)\n",
    "            val_roc[\"Dataset\"] = \"val\"\n",
    "            val_roc[\"order\"] = val_roc.index\n",
    "      \n",
    "            ##　for test set\n",
    "            fpr, tpr, thresholds, test_roc_auc = S4.ROC_AUC(test_label, test_predict_prob)\n",
    "            test_roc = OrderedDict({\"fpr\": fpr[1], \"tpr\": tpr[1], \"thresholds\": thresholds[1]})\n",
    "            test_roc = pd.DataFrame.from_dict(test_roc)\n",
    "            test_roc[\"Dataset\"] = \"test\"  \n",
    "            test_roc[\"order\"] = test_roc.index\n",
    "    \n",
    "            roc_df = pd.concat([val_roc, test_roc], axis = 0, ignore_index=True)\n",
    "            roc_df[\"K-fold\"] = k\n",
    "            roc_df[\"Trial\"] = trial\n",
    "            roc_df[\"Focal_gamma\"] = gamma\n",
    "            roc_df[\"Focal_alpha0\"] = alpha0\n",
    "            \"\"\"\n",
    "            del val_predict_prob, val_predict_label, test_predict_prob, test_predict_label\n",
    "            #%%                                      V.4 Evaluation for metrics: Intermediate data\n",
    "            #Prediction of Intermediate data for validation set\n",
    "            val_new_predict_prob = model.predict(val_data_INTER)\n",
    "            \n",
    "            if Activation == 'sigmoid':\n",
    "                ### Row normalization (Prob sum up t0 1)\n",
    "                val_new_predict_prob = val_new_predict_prob/val_new_predict_prob.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            RMSE = math.sqrt( ((val_label_INTER - val_new_predict_prob)**2).sum()/len(val_new_predict_prob) )\n",
    "            \n",
    "            #Prediction of Intermediate data for testing set\n",
    "            test_new_predict_prob = model.predict(test_data_INTER) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            if Activation == 'sigmoid':\n",
    "                ### Row normalization (Prob sum up t0 1)\n",
    "                test_new_predict_prob = test_new_predict_prob/test_new_predict_prob.sum(axis=1)[:, np.newaxis]\n",
    "                \n",
    "            RMSE_test = math.sqrt( ((test_label_INTER - test_new_predict_prob)**2).sum()/len(test_new_predict_prob) )\n",
    "\n",
    "            inter_dict = OrderedDict({\"Focal_gamma\": gamma, \"Focal_alpha0\": alpha_list[0], \"Focal_alpha1\": alpha_list[1],\n",
    "                                      \"N1\": N_exp, \"N2\": N_exp2, \"N3\": N_exp3, \"N4\": N_exp4,\n",
    "                                      \"Trial\": trial, \"K-fold\": k, \"RMSE_val\": RMSE, \"RMSE_test\": RMSE_test})\n",
    "            for ii in range(len(Labels)):\n",
    "                inter_dict[f'Mean_P{str(ii)}_val'] = val_new_predict_prob[:,ii].mean()\n",
    "            for ii in range(len(Labels)):\n",
    "                inter_dict[f'Mean_P{str(ii)}_test'] = test_new_predict_prob[:,ii].mean()  \n",
    "\n",
    "            ## save INTER testing set prediction (of each fold of model)  -------------------------------------------\n",
    "            test_new_predict_prob_df = pd.DataFrame(test_new_predict_prob, columns=Labels)\n",
    "            test_new_predict_prob_df[\"accuracy\"] = overall_val['accuracy']\n",
    "            test_new_predict_prob_df[\"f1-score_macro\"] = overall_val['f1-score_macro']\n",
    "            test_new_predict_prob_df[\"RMSE\"] = RMSE\n",
    "            test_new_predict_prob_df[\"Trial\"] = trial\n",
    "            test_new_predict_prob_df[\"K-fold\"] = k\n",
    "            \n",
    "            # Calculate mean prob. for differenct INTER data -------------------------------------------------------------\n",
    "            ##for validation set\n",
    "            val_new_predict_prob = pd.DataFrame(val_new_predict_prob)\n",
    "            val_new_predict_prob.columns = ['Pr_'+i for i in Labels]\n",
    "            val_new_predict_prob['C0>C1'] = (val_new_predict_prob.iloc[:,0] > val_new_predict_prob.iloc[:,1])*1\n",
    "            val_new_predict_prob['C0<C1'] = 1-val_new_predict_prob['C0>C1']\n",
    "            val_new_predict_prob['INTER_type'] = 0\n",
    "            val_INTER_n[0] = 0\n",
    "            val_INTER_n_cum = pd.DataFrame([val_INTER_n]).cumsum(axis=1).values\n",
    "            for i in range(len(val_INTER_n)-1): \n",
    "                val_new_predict_prob.loc[ val_INTER_n_cum[0, i]:val_INTER_n_cum[0, i+1],'INTER_type'] = i+1\n",
    "                \n",
    "            val_M_INTER = val_new_predict_prob.groupby(['INTER_type']).mean()\n",
    "            \n",
    "            ## for testing set\n",
    "            test_new_predict_prob = pd.DataFrame(test_new_predict_prob)\n",
    "            test_new_predict_prob.columns = ['Pr_'+i for i in Labels]\n",
    "            test_new_predict_prob['C0>C1'] = (test_new_predict_prob.iloc[:,0] > test_new_predict_prob.iloc[:,1])*1\n",
    "            test_new_predict_prob['C0<C1'] = 1-test_new_predict_prob['C0>C1']\n",
    "            test_new_predict_prob['INTER_type'] = 0\n",
    "            test_INTER_n[0] = 0\n",
    "            test_INTER_n_cum = pd.DataFrame([test_INTER_n]).cumsum(axis=1).values\n",
    "            for i in range(len(test_INTER_n)-1): \n",
    "                test_new_predict_prob.loc[ test_INTER_n_cum[0, i]:test_INTER_n_cum[0, i+1],'INTER_type'] = i+1\n",
    "                \n",
    "            test_M_INTER = test_new_predict_prob.groupby(['INTER_type']).mean()            \n",
    "            \n",
    "            \n",
    "            val_M_INTER['dataset'] = 'val'\n",
    "            test_M_INTER['dataset'] = 'test'\n",
    "\n",
    "            # concate results\n",
    "            M_INTER = pd.concat([val_M_INTER, test_M_INTER], axis =0)\n",
    "            M_INTER[\"Focal_gamma\"] = gamma\n",
    "            M_INTER[\"Focal_alpha0\"] = alpha_list[0]\n",
    "            M_INTER[\"Focal_alpha1\"] = alpha_list[1]\n",
    "            M_INTER[\"N1\"] = N_exp\n",
    "            M_INTER[\"N2\"] = N_exp2\n",
    "            M_INTER[\"N3\"] = N_exp3\n",
    "            M_INTER[\"N4\"] = N_exp4\n",
    "\n",
    "            M_INTER[\"Trial\"] = trial\n",
    "            M_INTER[\"K-fold\"] = k\n",
    "            \n",
    "            \n",
    "            del overall_val, overall_test \n",
    "            del val_M_INTER, test_M_INTER, val_new_predict_prob, test_new_predict_prob\n",
    "\n",
    "            #%%                                        VII. Record and Output to files\n",
    "            #%%\n",
    "            # Export all results directory after all iterations\n",
    "            with open(os.path.join(output_path, f'Evaluation_{Prep}{other_info}_inter_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF_inter:\n",
    "                pd.DataFrame([inter_dict]).to_csv(evalF_inter, mode='a',header = evalF_inter.tell()==0, index=False)\n",
    "            \n",
    "            with open(os.path.join(output_path, f'Evaluation_{Prep}{other_info}_overall_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF_overall:\n",
    "                overall.to_csv(evalF_overall, mode='a',header = evalF_overall.tell()==0, index=False)\n",
    "                \n",
    "            with open(os.path.join(output_path , f'Evaluation_{Prep}{other_info}_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF:\n",
    "                report.to_csv(evalF, mode='a', header = evalF.tell()==0, index=True)\n",
    "        \n",
    "            with open(os.path.join(output_path, f'History_{Prep}{other_info}_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as historyF:\n",
    "                train_history.to_csv(historyF, mode='a', header = historyF.tell()==0, index=False)\n",
    "\n",
    "            with open(os.path.join(output_path , f'Evaluation_{Prep}{other_info}_inter_M_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF_inter_M:\n",
    "                M_INTER.to_csv(evalF_inter_M, mode='a', header = evalF_inter_M.tell()==0, index=True)\n",
    "\n",
    "            if Prep == \"DS_INTER\":\n",
    "                with open(os.path.join(output_path, f'DS_N_{Prep}{other_info}_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as DS_F:\n",
    "                    DS_N.to_csv(DS_F, mode='a', header = DS_F.tell()==0, index=False)\n",
    " \n",
    "            with open(os.path.join(output_path , f'Evaluation_{Prep}{other_info}_TestPred_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF_test:\n",
    "                test_predict_prob_df.to_csv(evalF_test, mode='a', header = evalF_test.tell()==0, index=True)\n",
    "            with open(os.path.join(output_path , f'Evaluation_{Prep}{other_info}_TestPred_inter_batch-{batch_size}_epoch-{epochs}.csv'), \"a\", newline='\\n') as evalF_test:\n",
    "                test_new_predict_prob_df.to_csv(evalF_test, mode='a', header = evalF_test.tell()==0, index=True)\n",
    "       \n",
    "        del train_skf_data, val_skf_data, model, callbacks_list, model_history, train_history, report, M_INTER, test_predict_prob_df, test_new_predict_prob_df\n",
    "\n",
    "    del train_data, test_data, train_label, test_label, train_index, test_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Output completed, time spent (in hours):\", (end_time - start_time)/3600)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#%%                                        VIII. Model prediction on new testing set\n",
    "#%%\n",
    "\n",
    "#%%                                  I.1 Load trained model from .h5\n",
    "from keras.models import load_model\n",
    "# for model\n",
    "#from CNN_Procedure import S3_Modeling as S3\n",
    "#model = load_model(f'EfficientNetB3_lung_TVGH_{data_ver}.h5') # without customized loss function\n",
    "\n",
    "# For customized loss funciton (before loading model)\n",
    "trial = 0\n",
    "k= 0\n",
    "\n",
    "#import keras.losses\n",
    "#keras.losses.focal_loss = S3.FocalLoss(gamma, alpha_list)\n",
    "\n",
    "model = load_model(os.path.join(output_path, f'EfficientNetB3_NoneNone_t{str(trial)}_K{str(k)}.h5'))\n",
    "#or other options\n",
    "#model_3 = load_model(os.path.join(output_path, 'EfficientNetB3_NoneNone_b16_e5_t1_K3_a0.82_g5.h5'), custom_objects={'FocalLoss':S3.FocalLoss(gamma, alpha_list)})\n",
    "\n",
    "'''\n",
    "if you wish to just perform inference with your model and not further optimization or training your model,\n",
    " you can simply wish to ignore the loss function like this: model_2\n",
    "model_2 = load_model(os.path.join(output_path, 'EfficientNetB3_NoneNone_b16_e5_t1_K3_a0.82_g5.h5'), compile=False)\n",
    "'''\n",
    "\n",
    "#%%                                   I.2 Load testing set\n",
    "H, W =  300, 300 # 224,224# Input shape, defined by the model (model.input_shape)\n",
    "resize = (H, W) \n",
    "\n",
    "data_folder = 'oral cancer inter stage-all_tag300'\n",
    "origion_path = \"D:\\\\Project_Jane\\\\StemCellReg2021\\\\Data\\\\\" + data_folder\n",
    "\n",
    "\n",
    "test_new_path = pd.read_csv(os.path.join(output_path, 'test_index_INTER_NoneNone_t0.csv'), header=None).values\n",
    "test_new_path = [i[0] for i in test_new_path.tolist()]\n",
    "\n",
    "\n",
    "Labels_INTER = ['TG2-WT1_0709', '20X_TG2-WT1_1021', '20X_TG1-WT2_1021'] #soft labels = [[1/3, 2/3, 0], [1/3, 2/3, 0], [2/3, 1/3, 0]]\n",
    "test_new_data= []\n",
    "for j, i in enumerate(test_new_path):\n",
    "    print(j, i)\n",
    "    if j < 40:\n",
    "        Label_INTER = Labels_INTER[1]\n",
    "    else:\n",
    "        Label_INTER = Labels_INTER[2]\n",
    "    print('current searched folder:', Label_INTER)    \n",
    "    test_image = cv2.imread(os.path.join(origion_path, Label_INTER, i))[:,:,::-1]\n",
    "    #test_image = cv2.resize(test_image, resize )\n",
    "    test_new_data.append(test_image)\n",
    "test_new_data= np.array(test_new_data)\n",
    "\n",
    "\n",
    "#%%                                             I.1 Predition\n",
    "#%% prediction\n",
    "test_predict_prob = model.predict(test_new_data)\n",
    "test_predict_label = np.argmax(test_predict_prob,axis=1)\n",
    "\n",
    "\n",
    "test_predict_prob = pd.DataFrame(test_predict_prob)\n",
    "test_predict_prob.columns = ['Pr_'+i for i in Labels]\n",
    "test_predict_prob.index = test_new_path\n",
    "\n",
    "test_predict_prob.to_csv(os.path.join(output_path, f'Pred_Prob_Oral_INTER_Data_{Prep}{other_info}_t{trial}_K{k}.csv'))\n",
    "\n",
    "#%% summary statistics\n",
    "test120 = test_predict_prob.iloc[:40]\n",
    "test210 = test_predict_prob.iloc[40:]\n",
    "print(test120.mean())\n",
    "print(test210.mean())\n",
    "\n",
    "print(\"test120: ratio of Pr_5nMTG>Pr_control:\\t\",sum(test120['Pr_5nMTG']> test120['Pr_control'])/40)\n",
    "print(\"test210: ratio of Pr_control>Pr_5nMTG:\\t\",sum(test210['Pr_control']>test210['Pr_5nMTG'])/40)\n",
    "\n",
    "#%% histogram\n",
    "print(\"Mean probabilities:\\n\", test_predict_prob.mean(axis=0))\n",
    "M = test_predict_prob['Pr_control'].mean()\n",
    "\n",
    "   \n",
    "\n",
    "eps = [0, -2, 0]    \n",
    "colors = ['blue', 'orange', 'green']\n",
    "plt.figure(figsize=(10,7))\n",
    "for i, col in enumerate(test_predict_prob.columns):\n",
    "    M = test_predict_prob[col].mean()\n",
    "    test_predict_prob[col].hist(bins = 20, color = colors[i], range=[0,1], label =col, alpha =0.5) #bins=10, range=[0,0.5], color ='orange'\n",
    "    plt.axvline(M, color = colors[i])\n",
    "    plt.annotate(f'Mean={str(round(M,4))}',xy = (M, 0), xytext = (M, -4+eps[i]), arrowprops =dict(facecolor=colors[i])) #(M-0.06, -10)\n",
    "plt.title(f\"Histogram of Prob\")  \n",
    "plt.legend() \n",
    "plt.savefig(os.path.join(output_path, f'Histogram of Prob_t{trial}_K{k}.png'), transparent=True)\n",
    "plt.close()   \n",
    "\n",
    "    \n",
    "colors = ['blue', 'orange', 'green']\n",
    "plt.figure(figsize=(10,16))\n",
    "plt.subplot(2,1,1)\n",
    "eps = [0, 0, 0]\n",
    "for i, col in enumerate(test_predict_prob.columns):\n",
    "    M = test120[col].mean()\n",
    "    test120[col].hist(bins = 20, color = colors[i], range=[0,1], label =col, alpha =0.5) #bins=10, range=[0,0.5], color ='orange'\n",
    "    plt.axvline(M, color = colors[i])\n",
    "    plt.annotate(f'Mean={str(round(M,4))}',xy = (M, 0), xytext = (M, -3+eps[i]), arrowprops =dict(facecolor=colors[i]), fontsize=6) #(M-0.06, -10)\n",
    "plt.title(f\"Histogram of Prob (Control: 5nMTG=1:2)\")  \n",
    "plt.legend() \n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "eps = [0, 0, 0]\n",
    "for i, col in enumerate(test_predict_prob.columns):\n",
    "    M = test210[col].mean()\n",
    "    test210[col].hist(bins = 20, color = colors[i], range=[0,1], label =col, alpha =0.5) #bins=10, range=[0,0.5], color ='orange'\n",
    "    plt.axvline(M, color = colors[i])\n",
    "    plt.annotate(f'Mean={str(round(M,4))}',xy = (M, 0), xytext = (M, -3+eps[i]), arrowprops =dict(facecolor=colors[i]), fontsize=6) #(M-0.06, -10)\n",
    "plt.title(f\"Histogram of Prob (Control: 5nMTG=2:1)\")  \n",
    "plt.legend() \n",
    "plt.savefig(os.path.join(output_path, f'Histogram of Prob_t{trial}_K{k}_subplots.png'), transparent=True)\n",
    "plt.close() \n",
    "\n",
    "\n",
    "#%% stacked-Bar Plot            # Check if modification needed!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "test_predict_prob2 = test_predict_prob#.sort_values(['Pr_control'])\n",
    "    \n",
    "index = list(range(len(test_predict_prob2)))\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(index, test_predict_prob2['Pr_control'], label='Pr_control', align='edge', width=1.0)\n",
    "ax.bar(index, test_predict_prob2['Pr_5nMTG'], bottom=test_predict_prob2['Pr_control'], label=\"Pr_5nMTG\", align='edge', width=1.0)\n",
    "ax.bar(index, test_predict_prob2['Pr_NonCancerHEK293T'], bottom = test_predict_prob2['Pr_control'] + test_predict_prob2['Pr_5nMTG'], label=\"Pr_NonCancerHEK293T\", align='edge', width=1.0)\n",
    "ax.legend(loc = 'lower right') #'best'\n",
    "#plt.show()\n",
    "plt.savefig(os.path.join(output_path, f'Stacked bar plot_t{trial}_K{k}.png'), transparent=True)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "'''\n",
    "#%%                                        IX. Preprocessing Visual Illustration\n",
    "#%%\n",
    "# illustrate image interpolation (for SMOTE / Mixup)\n",
    "directory = \"D:\\Project_Jane\\StemCellReg2021\\Data\"\n",
    "X = cv2.imread(os.path.join(directory, \"oral cancer inter stage-all_tag300\\\\20X_TG1-WT2_1021\\\\inter210_00.tif\"))[:,:,::-1]\n",
    "X_nn = cv2.imread(os.path.join(directory,\"oral cancer 0521-0618_tag300\\\\control\\\\control_00.tif\"))[:,:,::-1]\n",
    "\n",
    "r = 0.24\n",
    "X_Mixup = X + r*(X_nn-X) \n",
    "temp = np.round(X_Mixup,0)\n",
    "temp=temp.astype(np.int_)\n",
    "\n",
    "plt.imshow(temp)\n",
    "plt.axis('off') \n",
    "plt.savefig(\"Mixup inter210_00 vs control_00.png\", transparent = True)\n",
    "'''\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
